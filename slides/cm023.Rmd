---
title: "Math 530/630 CM 2.3"
subtitle: "Coding Style and Linear Models"
output:
  xaringan::moon_reader:
    css: ["default", "robot", "robot-fonts", "css/ohsu.css", "css/ohsu-fonts.css"]
    header-includes:
    - \usepackage[amsmath]{amsmath}
    lib_dir: libs
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(moderndive)
library(extrafont)
library(broom)
```

```{r data, include = FALSE}
crimenames <- c("county", "region_name", "region_code",
               "criminals", "public_houses", "school_attendance",
               "worship_attendance")

crime <- read_table(here::here("data", "beerhall.dat"),
                    col_names = crimenames)
# For ggplot2
 ggplot2::theme_set(ggplot2::theme_minimal())
#
# !! Using extrafont::element_text takes a bunch more work see here: https://github.com/wch/extrafont/blob/master/README.md
#                   + ggplot2::theme(text = element_text(family = "Lato")))
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warn = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 3, fig.width = 5.5, dpi = 300,
                      warning = FALSE, message = FALSE)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
```

class: center, middle

# Coding style

---

## Style guide

>"Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread."
>
>Hadley Wickham

- Style guide for this course is based on the Tidyverse style guide: http://style.tidyverse.org/

---
#lintr and styler
.pull-left[
[lintr](https://github.com/jimhester/lintr) 
- R package to check formatting
  - CRAN: *install.packages("lintr")*, OR
  - Dev version (better integrated): *devtools::install_github("jimhester/lintr")* 
]
.pull-right[
[styler](https://styler.r-lib.org/) 
  - R package to perform formatting
  - *install.packages("styler")*
]
---
class: inverse, middle, center
## Linear models

---

## Vocabulary

- **Outcome variable:** Observed variable whose behavior or variation you are trying to understand, on the y-axis (aka dependent or response variable)

--

- **Explanatory variables:** Other observed variables that you want to use to explain the variation in the outcome, on the x-axis (aka independent or predictor variables)

--

- **Fitted values:** Output of the **model function** (aka predicted values)
    - The model function gives the typical value of the response variable
    *conditioning* on the explanatory variables

--

- **Residuals:** Show how far each case is from its fitted model value
    - Residual = Observed outcome value - Fitted value
    - Tells how far above/below the model function each case is

---
## The conceptual linear model

[ModernDive](http://moderndive.netlify.com/6-regression.html#model1table) presented us with this formula:

$$\widehat{y} = b_0 + b_1 \cdot x$$

Where:

* the intercept coefficient is $b_0$, or the value of $\widehat{y}$ when $x=0$, and 
* the slope coefficient $b_1$, or the increase in $\widehat{y}$ for every increase of one in $x$.


$$\downarrow$$
<br>
--

$$\overbrace{\widehat{y}}^{fitted} = \overbrace{b_0}^{intercept} + \overbrace{b_1}^{slope} \cdot x$$

???

Notation:  
  - hat means "fitted".

---
class: center, middle
## The linear model in R

$$\widehat{y} = b_0 + b_1 \cdot x$$
<br>
--

$$\downarrow$$
<br>
--

$$y = \overbrace{b_0 + b_1 \cdot x}^{\hat{y}} + e$$
<br>
--

$$\downarrow$$

<br>
--

```{r eval = FALSE}
lm(y ~ x, data = dataframe) 
```

???
- But this only describes the points that are ON the line. 
- To model data, we need to also explain the relationship of the line to the data points that are off the line  
- often called `error`,and can be (e.g. measuerment), but is expected - examples??

Notation
- ~, think as explained by...  
- the b's are *least squares estimators* 

---
## `lm` in R

```{r}
(crime_from_ph <- lm(criminals ~ public_houses, data = crime))
```


--

<br>

$$\widehat{\textrm{criminals}} = 109.34 + .12~\textrm{public_houses}$$
--

- **Slope:** For each additional public house per 100k people, the crime per 100k people is expected to be higher, on average, by 0.12.

--

- **Intercept:** Counties with 0 public houses are expected to have crime rates at 109.34 per 100k people, on average.


???
So, let's see if it matches what we'd expect given our EDA

---
## Interpreting `lm` in R


```{r basic_lm, echo=FALSE, out.width='70%'}
ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", colour = "dodgerblue", se = FALSE) 
```


$$\widehat{\textrm{criminals}} = 109.34 + .12~\textrm{public_houses}$$


- **Slope:** For each additional public house per 100k people, the crime per 100k people is expected to be higher, on average, by 0.12.



- **Intercept:** Counties with 0 public houses are expected to have crime rates at 109.34 per 100k people, on average.
    - Does this make sense?


??? 
 - Does the line describe what we see in the data? And match the correlation we saw in the lab?
 - What about at x=0? 

---
## Intercept

- **Intercept:** Counties with 0 public houses are expected to have crime rates at 109.34 per 100k people, on average.
    - Does this make sense? 
    - Do we have any counties with 0?
    
```{r}
crime %>% 
  filter(public_houses == 0)
```

While the intercept has a mathematical interpretation (it situates the vertical location of the line), it may not always have a practical one.


--

A solution we'll talk more about is ["centering" your variables](https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2010.00012.x) before using `lm`.

---

## Properties of the least squares regression line

- The regression line goes through the center of mass point, the coordinates corresponding to average $x$ and average $y$: $(\bar{x}, \bar{y})$:

$$\hat{y} = b_0 + b_1 x ~ \rightarrow ~ b_0 = \hat{y} - b_1 x$$

- The slope has the same sign as the correlation coefficient:

$$b_1 = r \frac{s_y}{s_x}$$

- The sum of the residuals is zero: 
$$\sum_{i = 1}^n e_i = 0$$

<!-- - The residuals and $x$ values are uncorrelated. well, they should be-->
---
## Fitted and residual values

```{r}
(crime_pts <- get_regression_points(crime_from_ph))
```

---
## Fitted values

What do you see?

```{r echo=FALSE}
ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", colour = "dodgerblue", se = FALSE) + 
  coord_cartesian(xlim = c(0, 1000)) +
  scale_x_continuous(expand=c(0,0), limits=c(0,1000))
```


---

## Fitted values

Caution: extrapolating...

```{r echo=FALSE}
ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", colour = "dodgerblue", se = FALSE, 
              fullrange = TRUE) + 
  coord_cartesian(xlim = c(0, 1000)) +
  scale_x_continuous(expand=c(0,0), limits=c(0,1000))
```
---
class:center middle
#[xkcd](https://xkcd.com/)

![](https://imgs.xkcd.com/comics/extrapolating.png)


---
class: center, middle
## Looking at regression another way ...



$$\overbrace{y_i}^{observed} = \overbrace{b_0 + b_1 \cdot x_i}^{fitted} + \overbrace{e_i}^{residual}$$
<br>
--
$$\downarrow$$
<br>
--
$$\overbrace{y_i}^{outcome} = \overbrace{b_0 + b_1 \cdot x_i}^{model} + \overbrace{e_i}^{error}$$
<br>
--
$$\downarrow$$
--
$$\textrm{outcome}_i = \textrm{(Model}_i) + \textrm{error}_i$$
--

> *"All models are wrong but some are useful"* -George Box

---
class: middle, inverse, center

## So, how useful is your simple linear regression model?

--

1. Create a scatterplot with the residuals on the $y$-axis and the original explanatory variable $x$ on the $x$-axis.

--

2. Create a histogram of the residuals, thereby showing the *distribution* of the residuals.

--

## What are we looking for?

---

## Variation around the model...

is just as important as the model, if not more!

*Statistics is the explanation of variation in the context of what remains
unexplained.*

- The scatter suggests that there might be other factors that account for large parts 
of country-to-county variability, or perhaps just that randomness plays a big role.

- Adding more explanatory variables to a model can sometimes usefully reduce
the size of the scatter around the model. (We'll talk more about this later.)


---
## Scatterplot of the residuals<sup>1</sup>

"1. the residuals should be on average 0. In other words, sometimes the regression model will make a positive error in that $y - \widehat{y} > 0$, sometimes the regression model will make a negative error in that $y - \widehat{y} < 0$, but *on average* the error is 0." 

```{r echo = FALSE, out.width='75%'}
ggplot(crime_pts, aes(x = public_houses, y = residual)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 3, lwd = 3, color = "turquoise") + 
  geom_hline(aes(yintercept = mean(residual)), 
             color = "dodgerblue")
```



.footnote[
[1] From [ModernDive](http://moderndive.netlify.com/6-regression.html#model1residuals)

]

???
- Different scatter shapes (NOT heteroscedastic)
- Skewed (need some big and some little) 


---
## Scatterplot of the residuals<sup>1</sup>

"2. Further, the value and spread of the residuals should not depend on the value of $x$."

```{r echo = FALSE, out.width='75%'}
ggplot(crime_pts, aes(x = public_houses, y = residual)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 3, lwd = 3, color = "turquoise") + 
  geom_smooth(color = "dodgerblue")
```



.footnote[
[1] From [ModernDive](http://moderndive.netlify.com/6-regression.html#model1residuals)
]

---
## Histogram of the residuals<sup>1</sup>

"we would like the residuals to be normally distributed with mean 0. In other words, be bell-shaped and centered at 0!"

```{r, out.width='75%'}
ggplot(crime_pts, aes(x = residual)) +
  geom_histogram(binwidth = 18, color = "white") 
```

.footnote[
[1] From [ModernDive](http://moderndive.netlify.com/6-regression.html#model1residuals)
]

---
class: center, middle
## Modeling 


$$\overbrace{y}^{outcome} = \overbrace{b_0 + b_1 \cdot x}^{model} + \overbrace{e}^{error}$$

<br>
--

$$\downarrow$$

<br>
--

$$\textrm{outcome}_i = \textrm{(Model}_i) + \textrm{error}_i ~ \rightarrow ~ \textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$
<br>
--

$$\downarrow$$

<br>
--

> *"All models are wrong but some are useful"* -George Box


???
- Another way of looking at the error 
- how far off is your model from the actual data?
---
class: middle, inverse, center


## So, how useful is your simple linear regression model?

We are now going to answer this a different way...


---
## Model and Outcome

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

1. What is the "outcome"?
1. What is the "model"?

```{r fitted_model, echo = FALSE}
ggplot(crime, aes(public_houses, criminals)) +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue") +  
  geom_point(aes(x = public_houses, 
                 y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21)  
```



---
## Model and Outcome

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

1. What is the "outcome"? $y_i$ (dark blue dots)
1. What is the "model"? $\hat{y_i}$ (bright blue dots)

![](`r knitr::fig_chunk('fitted_model','png')`)



---
## What is the "model" saying?

- The correlation coeeficient between `public_houses` and `criminals` is `r round(cor(crime$criminals, crime$public_houses), 2)` 
- What would r be using only the bright blue dots?

![](`r knitr::fig_chunk('fitted_model','png')`)

???
An exact association
---
## Sums of Squares Residual

The total amount of error present when the best-fitting linear model is applied to the data. 

$$SS_{residual} = \sum{e_i^2} = \sum{(y_i - \hat{y_i})^2}$$

```{r ss_resid, echo = FALSE, out.width='75%'}
ss_resid <- ggplot(crime, aes(public_houses, criminals)) +
  geom_segment(aes(x = public_houses, 
                   xend = public_houses, 
                   y = criminals, 
                   yend = predict(lm(criminals ~ public_houses))), 
               colour = "red") +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue") +  
  geom_point(aes(x = public_houses, 
                 y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21)  
ss_resid
```


???
Does this look familiar (kinda like variance)

---
## Least squares regression

The regression line minimizes the sum of squared residuals.

$$\hat{y} = b_0 + b_1~x$$
What is another (equivalent) way to say the same thing?

--

$$y_i = b_0 + b_1~x_i + e_i$$

$$y_i = \hat{y_i} + e_i$$

--

If $e_i = y_i - \hat{y_i}$,

then, the regression line minimizes $SS_{residual} = \sum_{i = 1}^n e_i^2$.

---
class: middle, inverse, center

## The `SS Residual` compares the best-fitting linear model to the *true (unknown) data-generating model*

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

$$SS_{residual} = \sum{e_i^2} = \sum{(y_i - \hat{y_i})^2}$$
---

## Can you spot another model?

```{r echo = FALSE}
crime_scatter <- ggplot(crime, aes(public_houses, criminals)) +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue") +  
  geom_point(size = 2, color = "navy", alpha = .8) 
crime_scatter
```

---
## Hint

How many criminals would you predict are in county X, if you don't know how many public houses there are?

![](`r knitr::fig_chunk('basic_lm','png')`)
---

## The mean as a model

```{r echo = FALSE}
crime_scatter +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") 
```
---
## The mean as the "null" model

"The turquoise points are what we would expect to see if there was `___` association between `public_houses` and `criminals`"

```{r mean_model, echo = FALSE}
crime_scatter +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) 
```


---
## If the mean is the model...

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

1. What is the "outcome"? 
1. What is the "model"? 

--

![](`r knitr::fig_chunk('mean_model','png')`)

---
## If the mean is the model...

$$\textrm{error}_i = \textrm{outcome}_i - \textrm{(Model}_i)$$

1. What is the "outcome"? $y_i$ (dark blue dots)
1. What is the "model"? $\bar{y}$ (turquoise dots)


![](`r knitr::fig_chunk('mean_model','png')`)

---
## What is this "model" saying?

_(Ignore the regression line for now)_

What would the magnitude of the differences between the observed `criminals` values and the mean value tell us? *(remember what we said the mean line represented: `___` correlation between variables)*

```{r ss_total, echo = FALSE}
ss_total <- ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue", alpha = .2) +  
  geom_segment(aes(x = public_houses, 
                   xend = public_houses, 
                   y = criminals, 
                   yend = mean(criminals)), 
               colour = "orchid") +
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) +
  labs(subtitle = NULL, title = NULL)
ss_total
```

???
Define the SST equation together?

---
## What about spread?

Variance is calculated by taking the sum of squared differences from the mean, and dividing by `N-1`


```{r echo = FALSE}
crime %>% 
  summarize(var_crime = var(criminals),
            sd_crime = sd(criminals))
```

--


```{r}
crime %>% 
  summarize(sst_crime = sum((criminals - mean(criminals))^2),
            var_crime = sst_crime / (n() - 1),
            sd_crime = sqrt(var_crime))
```


---
## Take another look

The sum of squared differences between the `___` and the `___` values of `criminals` is the numerator of the variance (said another way, divide by `N-1` to get `var(criminals)`)

--

```{r echo = FALSE}
ss_total
```


---

## Sums of Squares Total

The __"Sums of Squares Total"__ is the total amount of error present when the most basic model is applied to the data. 

--

What is the "most basic model"? The mean (aka null) model!

--

$$SS_{total} = \sum{(y_i - \bar{y})^2}$$

--

![](`r knitr::fig_chunk('ss_total','png')`)



---
## A thought experiment...3 "models"?

1. Zero association between the variables (aka, the mean / null model)
2. An exact association (b_1 = 0.1162) of between the variables
3. The true (but always unknown) data-generating model

```{r echo = FALSE}
ggplot(crime, aes(public_houses, criminals)) +
  geom_vline(aes(xintercept = min(public_houses)),
             lwd = 5, color = "yellow") +
  geom_smooth(method = "lm", se = FALSE, colour = "dodgerblue") +  
  geom_point(size = 2, color = "navy", alpha = .8) +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) +
  geom_point(aes(x = public_houses, 
             y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21) +
  labs(subtitle = NULL, title = NULL)
```


???
What do I think y is based on each model?


---
## A thought experiment...

Are there any other "differences" we can calculate with this set of 3 points/models?

```{r echo = FALSE}
crime_scatter +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) +
  geom_point(aes(x = public_houses, 
             y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21) +
  labs(subtitle = NULL, title = NULL)
```

---
## A thought experiment...(last!)

What about the difference between the mean and predicted values?

```{r echo = FALSE}
ss_model <- crime_scatter +
  geom_segment(aes(x = public_houses, 
                   xend = public_houses, 
                   y = predict(lm(criminals ~ public_houses)), 
                   yend = mean(criminals)), 
               colour = "turquoise") +
  geom_hline(aes(yintercept = mean(criminals)),
             color = "turquoise") +
  geom_point(aes(x = public_houses, 
             y = mean(criminals)),
             fill = "turquoise",
             size = 2,
             shape = 21) +
  geom_point(aes(x = public_houses, 
             y = predict(lm(criminals ~ public_houses))),
             fill = "dodgerblue",
             size = 2,
             shape = 21) +
  labs(subtitle = NULL, title = NULL)
ss_model
```

---
## A thought experiment...(last!)

The __"Sums of Squares Model"__ is the total amount of error present when the fitted values from the best-fitting linear model are compared to the fitted values from the most basic model.

```{r echo = FALSE}
ss_model
```

---
## All together now

.pull-left[
$$SS_{total} = SS_{model} + SS_{residual}$$

<br>

$$SS_{total}$$
```{r echo = FALSE}
ss_total
```

]

.pull-right[
$$SS_{model}$$
```{r echo = FALSE}
ss_model
```

$$SS_{residual}$$
```{r echo = FALSE}
ss_resid
```
]



---
## Summing the sums of squares

$$SS_{total} = SS_{model} + SS_{residual}$$


total variation = "explained" variation + residual ("left over") variation

---
## All of statistics

$$ \textrm{outcome}_i = \textrm{(Model}_i) + \textrm{error}_i$$

$$SS_{total} = SS_{model} + SS_{residual}$$

$$R^2 = SS_{model} / SS_{total}$$

--

also true...
$$R^2 = (1 - SS_{residual}) / SS_{total}$$
```{r}
library(broom)
glance(crime_from_ph)
```

