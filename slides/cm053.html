<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Math 530/630 CM 5.3</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css\ohsu.css" type="text/css" />
    <link rel="stylesheet" href="css\ohsu-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Math 530/630 CM 5.3
## Contrasts, post-hoc tests &amp; p-value adjustments

---










# Two key equations

When I introduced the *t*-test as a general linear model (GLM), I said that "all statistical procedures are basically the same thing":

`$$outcome_i=(model)+error_i$$`

Where:
* Outcome is your dependent variable (DV; also known as *y*) and
* Model is a linear function or linear combination of your independent variable(s) (IVs) (also known as *x*'s)

`$$DV_i=(model)+error_i$$`

"Essentially, all models are wrong, but some are useful"-George E. P. Box (1987)

`$$deviation=\sum{(observed-model)^2}$$`
???
* A couple minutes on linear models
* Not residuals, but expected error "in the world"
* Every statistic procedure we've looked at could be described by this function
* Let's look...
---

# The sample mean

.pull-left[
`\(DV_i=(1b)+error_i\)`

Coefficient=1 is implied.
]

.pull-right[
`\(DV_i=(model)+error_i\)`
]

???
The simplest model - the mean of the IV (no DV)
---

# Correlation

.pull-left[

`\(DV_i=(bIV_i)+error_i\)`

]

.pull-right[
`\(DV_i=(model)+error_i\)`
]

???
* Here the IV is some multiplier of the DV
* Still just a model
---

# Simple linear regression

.pull-left[

`\(DV_i=(b_0+b_1IV_i)+error_i\)`

Where:
* `\(b_0\)` is the intercept term and 
* `\(b_1\)` is the slope.
]

.pull-right[
`\(DV_i=(model)+error_i\)`
]

???
* Simple - meaning one IV
* Still just a model
---

# *t* test

.pull-left[

`\(DV_i=(b_0+b_1IV_i)+error_i\)`

Where:
* `\(b_0\)` is the intercept term ( `\(\bar{y}_{group1}\)` ) and 
* `\(b_1\)` is the slope ( `\(\bar{y}_{group2}-\bar{y}_{group1}\)` )
]

.pull-right[

`\(DV_i=(model)+error_i\)`
]

???
Now it's a linear model with dummy coded categories
---

# Analysis of Variance

.pull-left[

`\(DV_i=(b_0+b_1IV1_i+b_2IV2_i)+error_i\)`
]
.pull-right[

 `\(DV_i=(model)+error_i\)`
]

???
* Same as the t, but now may need more "dummy" codes


---

# GLM logic

* The simplest model we can ever conceive of for predicting a DV of interest is the mean of that DV.
* This is called the null (or reduced) model:
 * Simple linear regression: `\(y_i=b_0+\epsilon_i\)` 
 * ANOVA: `\(y_{ij}=\mu_{\bullet\bullet}+\epsilon_{ij}\)` where `\(\mu_{\bullet\bullet}\)` is the ___grand mean___

&gt; __N.B.__ What is *not* present in these equations?

???
In the simples model, the Dependent Variable does not matter
---

## Superhero ANOVA 



```r
superhero$hero &lt;- factor(superhero$hero,labels=c("Spiderman", "Superman", "Hulk", "TMNT"))
heroaov &lt;- aov(injury~hero,data=superhero)
summary(heroaov)
```

```
            Df Sum Sq Mean Sq F value  Pr(&gt;F)   
hero         3   3805  1268.3   6.716 0.00257 **
Residuals   20   3777   188.9                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
???
* Let's look at an example
* an anova(lm()) would give the same output
* What does this tell us?
  * A significant effect of the DV
---

## Now what?

&lt;img src="cm053_files/figure-html/unnamed-chunk-2-1.png" width="60%" /&gt;

???
* Maybe do nothing - well, put a chart
* Maybe drill down - doing post-hoc analyses
  * Important - anova and the post-hoc analyses answer two differenct questions
---

## Contrasts!

|Contrast |Spiderman|Superman|Hulk|TNMT|
|:------:|:------:|:-------:|:----:|:---:|
|Spiderman vs. Superman|1|-1|0|0|
|Spiderman vs. Hulk|1|0|-1|0|
|Hulk vs. TNMT|0|0|1|-1|


These say:
`$$+1\mu_{spiderman}-1\mu_{superman}+0\mu_{hulk}+0\mu_{TNMT}$$`
`$$+1\mu_{spiderman}+0\mu_{superman}-1\mu_{hulk}+0\mu_{TNMT}$$`
`$$0\mu_{spiderman}+0\mu_{superman}+1\mu_{hulk}-1\mu_{TNMT}$$`
???
* Contrasts are how R determines what to compare to what
* Matrix format
* Think back to linear models - each was an offset from the Intercept
* Model coefficients represent the differences specified by the contrasts

---

## Contrasts!

`$$+1\mu_{spiderman}-1\mu_{superman}+0\mu_{hulk}+0\mu_{TNMT}\rightarrow\mu_{spiderman}-\mu_{superman}=0$$`
`$$+1\mu_{spiderman}+0\mu_{superman}-1\mu_{hulk}+0\mu_{TNMT}\rightarrow\mu_{spiderman}-\mu_{hulk}=0$$`
`$$0\mu_{spiderman}+0\mu_{superman}+1\mu_{hulk}-1\mu_{TNMT}\rightarrow\mu_{hulk}-\mu_{TNMT}=0$$`
???
* More importantly, we're asking that R compute which of these means are significantly different

---

## Doing all pairwise contrasts in R


```r
pairwise.t.test(superhero$injury,superhero$hero,p.adjust.method="none")
```

```

	Pairwise comparisons using t tests with pooled SD 

data:  superhero$injury and superhero$hero 

         Spiderman Superman Hulk   
Superman 0.02222   -        -      
Hulk     0.26156   0.00165  -      
TMNT     0.12144   0.00056  0.64897

P value adjustment method: none 
```

**Base R's output=just the *p*'s, please!**


???

* Here's a full pairwise comparison
* Using pooled SD across all the observations


---

## Doing all pairwise contrasts in R (better)


```r
library(multcomp)
mcp &lt;- glht(heroaov,linfct=mcp(hero="Tukey")) # "Tukey" sets up the pairwise comparisons 
summary(mcp,test=adjusted("none")) #Same as pairwise.t.test() above 
```

You get:

* the *p*'s __plus__
* the *t*'s
* the `\(\psi\)`'s
* the `\(SE_{\psi}\)`'s
* oh my!

???
Tukey is not adjusting

---

## Doing all pairwise contrasts in R (better)


```

	 Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: aov(formula = injury ~ hero, data = superhero)

Linear Hypotheses:
                          Estimate Std. Error t value Pr(&gt;|t|)    
Superman - Spiderman == 0   19.667      7.934   2.479 0.022218 *  
Hulk - Spiderman == 0       -9.167      7.934  -1.155 0.261564    
TMNT - Spiderman == 0      -12.833      7.934  -1.617 0.121436    
Hulk - Superman == 0       -28.833      7.934  -3.634 0.001652 ** 
TMNT - Superman == 0       -32.500      7.934  -4.096 0.000562 ***
TMNT - Hulk == 0            -3.667      7.934  -0.462 0.648969    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Adjusted p values reported -- none method)
```

---

## Where are these numbers coming from?

`$$estimate=\psi={\sum_{j=1}^kc_j\bar{y_j}}$$`

where...
`$$SE_{est}=\hat{\sigma_{\psi}}=\sqrt{MS_{error}\times{(\frac{1}{n_1}+\frac{1}{n_2})}}$$`

And the *c* terms are contrast coefficients. Thus a more general formula is:

`$$t_{contrast}=\frac{estimate}{SE_{est}}=\frac{\psi}{\hat{\sigma_{\psi}}}=\frac{\sum_{j=1}^kc_j\bar{y_j}}{\sqrt{MS_{error}\times{\sum_{j=1}^k\frac{c_j^2}{n_j}}}}$$`
???
* c is the contrast
* Estimate is just the difference of the means
* SE is a weight adjusted MS error
* Higher n (in SE) -&gt; smaller denominator -&gt; larger t!
---


## This is different from the independent samples *t* test

`$$t=\frac{(\bar{y}_1-\bar{y}_2)-(\mu_1-\mu_2)}{\sqrt{\frac{(n_1-1)s_{y_1}^2+(n_2-1)s_{y_2}^2}{n_1+n_2-2}}}$$`


Which is usually just written as:
`$$t=\frac{(\bar{y}_1-\bar{y}_2)}{\sqrt{\frac{(n_1-1)s_{y_1}^2+(n_2-1)s_{y_2}^2}{n_1+n_2-2}}}$$`
???
*SE is just across two groups


---

## Contrast degrees of freedom

For the pairwise *t* tests ( `\(t_{contrast}\)` ), the degrees of freedom for __each__ contrast are the same: *N-k* (where *N*= total participants and *k*=number of groups)

If we were to conduct an independent samples *t* test, what would the degrees of freedom be? 
* The answer is: `\(n_1+n_2-2\)`
* This is the same as *N-k*, where *k* must always be 2 for the independent samples *t*-test.
* In our example contrast, this is the difference between 20 and 10 degrees of freedom



???
What happens when your df are higher?

---

## Increased degrees of freedom


|Degrees of Freedom| `\(\alpha=.05,\:2-tailed\)` |
|:-------:|:----------:|
|10|__2.228__|
|11|2.201|
|12|2.179|
|13|2.160|
|14|2.145|
|15|2.131|
|16|2.120|
|17|2.110|
|18|2.101|
|19|2.093|
|20|__2.086__|

---

## Defining contrast coefficients

We base the *c* terms off of the hypotheses. Let's take a look at the superhero data:

```
# A tibble: 4 x 4
  hero       mean    sd     n
  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
1 Spiderman  40.7 14.0      6
2 Superman   60.3 17.9      6
3 Hulk       31.5 12.8      6
4 TMNT       27.8  8.73     6
```

???
* Order matters
* matix size is dependent on the number of comparisons

---

## Defining contrast coefficients: contrast 1




|Contrast |Spiderman|Superman|Hulk|TNMT|
|:------:|:------:|:-------:|:----:|:---:|
|Spiderman vs. Superman|1|-1|0|0|
| `\(\bar{y_j}\)` |40.67|60.33|31.5|27.83|

`$$\psi=\sum_{j=1}^kc_j\bar{y_j}=(1)\times{40.67}+(-1)\times{60.33}=-19.667$$`

`$$\hat{\sigma_{\psi}}=\sqrt{188.85\times{(\frac{1}{6}+\frac{1}{6})}}=\sqrt{188.85\times{.333}}=7.934$$`

`$$t_{contrast}=\frac{\psi}{\hat{\sigma_{\psi}}}=\frac{-19.667}{7.934}=-2.479$$`


```r
2*(1-pt(2.479,20)) #unadjusted 2-tailed p-value; note the df!
```

```
[1] 0.02220591
```

???
* Walk through 1
* Compare spiderman to superman

---

## Another post-hoc option

The Tukey Honestly Significant Difference (HSD) is a single-step procedure that analyzes all possible pairwise contrasts between group means.

Instead of a *t* statistic ( `\(t_{contrast}\)` ), now we have:
* The HSD value for a family of contrasts
* The Tukey statistic, `\(q_{contrast}\)`, for each individual contrast within the family

???
* More conservative statitic

---

## Tukey Honestly Significant Difference (HSD)

`$$HSD=q_{tukey}\sqrt{\frac{MSE}{2}({\frac{1}{n_1}+\frac{1}{n_2}})}$$`


```r
#mse &lt;- 188.85
mse &lt;- summary(heroaov)[[1]]$"Mean Sq"[2] #I know, ugly!
ngroups &lt;- 4 #4 groups, so 4 means
dfmse &lt;- 20 #see ANOVA source table
q_tuk &lt;- qtukey(.95,ngroups,dfmse)
q_tuk
```

```
[1] 3.958293
```

`$$HSD=3.958\times\sqrt{\frac{188.85}{2}({\frac{1}{6}+\frac{1}{6}})}=22.36$$`

So, any mean difference larger than 22.36 we will consider significant according to Tukey HSD. 

---
##Studentized Range Distribution - _q_

![](images/StudentizedRangePDF.jpg)

---

## Which differences are &gt; than our HSD of 22.36?


```

	 Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: aov(formula = injury ~ hero, data = superhero)

Linear Hypotheses:
                          Estimate Std. Error t value Pr(&gt;|t|)   
Superman - Spiderman == 0   19.667      7.934   2.479  0.09405 . 
Hulk - Spiderman == 0       -9.167      7.934  -1.155  0.66075   
TMNT - Spiderman == 0      -12.833      7.934  -1.617  0.39176   
Hulk - Superman == 0       -28.833      7.934  -3.634  0.00836 **
TMNT - Superman == 0       -32.500      7.934  -4.096  0.00284 **
TMNT - Hulk == 0            -3.667      7.934  -0.462  0.96644   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Adjusted p values reported -- single-step method)
```
???
*Apparently Spiderman and superman aren't "honestly" different

---

## But wait!

### Those *p*-values are different than before!

Old `\(p\)`'s...


```
Superman - Spiderman     Hulk - Spiderman     TMNT - Spiderman 
        0.0222176879         0.2615640877         0.1214364334 
     Hulk - Superman      TMNT - Superman          TMNT - Hulk 
        0.0016520896         0.0005617408         0.6489687464 
```


New `\(p\)`'s...


```
Superman - Spiderman     Hulk - Spiderman     TMNT - Spiderman 
         0.094204135          0.660768079          0.391685358 
     Hulk - Superman      TMNT - Superman          TMNT - Hulk 
         0.008136062          0.002925340          0.966406140 
```


---

## Tukey HSD 

That's because the *p* value is based on a __new__ statistic ( `\(q_{contrast}\)` ), but only the *p* values in the table *glht* provides changed (which is confusing!). Let's see that new statistic...

For any specific contrast we can calculate:
`$$q_{contrast}=\frac{|\psi|}{\sqrt{\frac{MSE}{2}({\frac{1}{n_1}+\frac{1}{n_2}})}}$$`


```r
psi &lt;- abs(-19.667) 
se &lt;- sqrt((mse/2)*(1/6+1/6))
qobs &lt;- psi/se
se
```

```
[1] 5.610258
```

```r
qobs
```

```
[1] 3.505543
```

`$$q_{contrast}=\frac{19.667}{\sqrt{\frac{188.85}{2}({\frac{1}{6}+\frac{1}{6}})}}=3.506$$`

---

## Finding the *p*-value for Tukey HSD

So, according to Tukey HSD, we have the following:
`$$q_{Spiderman\:v.\:Superman}=3.506$$`

Because our critical `\(q_{tukey}\)` was 3.958, we know that our `\(q_{contrast}\)` of 3.506 is not significant. In fact, the *p*-value for our `\(q_{contrast}\)` is...


```r
ptukey(qobs,ngroups,dfmse,lower.tail=F)
```

```
[1] 0.09425035
```

* __N.B.__ #1: This is the same *p* value in the *glht* output.
* __N.B.__ #2: We reach the same conclusion based on comparing our difference in means to the HSD: for Spiderman vs. Superman, 19.667 &lt; 22.36.

---

## Note about Tukey HSD in *glht*

We have just observed something important in the *glht* output. 

* The *t* statistics correspond to our pairwise *t* test ( `\(t_{contrast}\)` ). This is a standard *t* distributed variable.
* The *p*-values correspond to the Tukey Studentized Range Distribution ( `\(q_{contrast}\)` ).

So in our example contrast between Spiderman and Superman:

|Distribution of Statistic|Statistic|SE|*p*-value|tails|
|:-------:|:----------:|:----:|:----:|:----:|
|Student *t* Distribution|2.479|7.93|.022|2 (can be 1)|
|Tukey Studentized Range Distribution|3.506|5.61|.094|always 1|


???
* True across glht in general??

---

## To sum up...

### Those *p*-values are different than before! Yes they are.

Old `\(p\)`'s based on Student *t* distributed statistic ( `\(t_{contrast}\)` )...


```
Superman - Spiderman     Hulk - Spiderman     TMNT - Spiderman 
        0.0222176879         0.2615640877         0.1214364334 
     Hulk - Superman      TMNT - Superman          TMNT - Hulk 
        0.0016520896         0.0005617408         0.6489687464 
```


New `\(p\)`'s based on Tukey Studentized Range statistic ( `\(q_{contrast}\)` )...


```
Superman - Spiderman     Hulk - Spiderman     TMNT - Spiderman 
         0.094400350          0.660726374          0.391775153 
     Hulk - Superman      TMNT - Superman          TMNT - Hulk 
         0.008441134          0.002949007          0.966422266 
```

---

## Why not just do a bunch of *t*-tests?

* First, note that pairwise contrasts use the degrees of freedom __based on all groups in your sample__, rather than for just the two groups in the contrast. This increases your statistical power!

* Second, pairwise contrasts (whether `\(t_{contrast}\)` or `\(q_{contrast}\)` ) base the standard error estimate (i.e., the denominator) on the weighted mean square error ( `\(MS_{error}\)` ) from the overall ANOVA. Again, this tends to increase power!

* But, each individual pairwise contrast is designed to control the probability of false rejection at `\(\alpha\)`. Unfortunately, if our data analysis involves many hypothesis tests (and thus many contrasts), the probability of at least one Type I error increases rather sharply with the number of contrasts.

???
* Tukey handles this somewhat - thus the changed p-values.
* Let's look at this more closely
---

## Probability(at least one error)

For example:
* If there are *m* tests
* They are independent
* Each is performed with `\(\alpha\)`=.05
* All alternative hypotheses are __true__
* What is the probability of at least one Type I error?

???
Example
* multiple tests cholesterol
* one treatment
* leads to a multiple comparison problem
---

## Probability refresher

If we have 3 levels of a factor, and want to compare all three to each other:
1. Contrast 1 vs. 2 ( `\(\alpha=.05\)` )
2. Contrast 2 vs. 3 ( `\(\alpha=.05\)` )
3. Contrast 1 vs. 3 ( `\(\alpha=.05\)` )

Probabilities:

* What is the probability that I will __incorrectly reject__ all three null hypotheses?
`$$P(\alpha)\times{P(\alpha)}\times{P(\alpha)}=(.05)(.05)(.05)=.000125$$`

* Using the same logic, what is the probability that I will __correctly reject__ all three null hypotheses?
`$$P(1-\alpha)\times{P(1-\alpha)}\times{P(1-\alpha)}=(.95)(.95)(.95)=.8574$$`
* Hmmm...

---

## Probability(at least one error)

 `$$P(Type \: I \: error)=\alpha$$`
 `$$P(no \: Type \: I \: error)=1-\alpha$$`
 `$$P(no \:Type\:I\:errors\:in\:m\:tests)=(1-\alpha)^m$$`
 `$$P(at\:least\:one\:Type\:I\:errors\:in\:m\:tests)=1-(1-\alpha)^m$$`


---

## Family-wise error rate (FWER)

The probability that I make __zero__ errors:
`$$P(1-\alpha)\times{P(1-\alpha)}\times{P(1-\alpha)}=(.95)(.95)(.95)=.8574$$`
The *family-wise error rate*:
`$$P(at\:least\:one\:error)=1-P(no\:errors)=1-.8574=.1426 &gt; .05$$`
 Thus, the probability that I will make at least one Type I error is &gt; .05

 `$$\alpha_{family}&gt;\alpha_{per\:test}$$`


---

## We can plot this...

&lt;img src="cm053_files/figure-html/unnamed-chunk-18-1.png" width="60%" /&gt;

---
class: middle, center
#What can we do?

--
##Adjust the p-values!
---

## Doing all pairwise contrasts in R better*


```r
summary(mcp, test = adjusted("BH")) #Tukey pairwise comparison + BH p-adjustment!
```

```

	 Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: aov(formula = injury ~ hero, data = superhero)

Linear Hypotheses:
                          Estimate Std. Error t value Pr(&gt;|t|)   
Superman - Spiderman == 0   19.667      7.934   2.479  0.04444 * 
Hulk - Spiderman == 0       -9.167      7.934  -1.155  0.31388   
TMNT - Spiderman == 0      -12.833      7.934  -1.617  0.18215   
Hulk - Superman == 0       -28.833      7.934  -3.634  0.00496 **
TMNT - Superman == 0       -32.500      7.934  -4.096  0.00337 **
TMNT - Hulk == 0            -3.667      7.934  -0.462  0.64897   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Adjusted p values reported -- BH method)
```

---

## But wait!

### You changed the *p*-values again! Yes I did.

`\(p\)`'s based on Tukey's HSD...


```
Superman - Spiderman     Hulk - Spiderman     TMNT - Spiderman 
         0.094400350          0.660726374          0.391775153 
     Hulk - Superman      TMNT - Superman          TMNT - Hulk 
         0.008441134          0.002949007          0.966422266 
```


`\(p\)`'s with Benjamini-Hochberg adjustment...


```
Superman - Spiderman     Hulk - Spiderman     TMNT - Spiderman 
         0.044435376          0.313876905          0.182154650 
     Hulk - Superman      TMNT - Superman          TMNT - Hulk 
         0.004956269          0.003370445          0.648968746 
```

---

## The Benjamini-Hochberg False Discovery Rate (FDR)

The basic idea of the FDR is to try to achieve the smallest possible fraction of false signals among all those that appear to be true (i.e., significant).

Said another way: we estimate the expected proportion of false rejections among all rejected null hypotheses and attempt to keep it under a threshold level.

Using the FDR method, we are trying to control the number of "false discoveries" rather than the number of "false positives." 

&gt; What is the difference?

---

## False Positives &amp; False Discoveries

Let's revisit the good ol' decision table. Let *m* be the total number of hypotheses tested.

|   |Decision: Do not reject `\(H_0\)` |Decision: Reject `\(H_0\)` | Total |
|:-:|:----:|:---:|:---:|
| `\(H_0\)` is true |U &lt;br&gt; *True Negative*|V &lt;br&gt; __Type I error__/*False Positive*|   `\(m_0\)` |
| `\(H_0\)` is false |T &lt;br&gt; __Type II error__/*False Negative*|S &lt;br&gt; *True Positive*|  `\(m-m_0\)`
| Total | `\(m-R\)` | `\(R\)` | `\(m\)` |

The false positive rate is:
`$$FPR=\frac{V}{m_0}$$`

The false discovery rate is:
`$$FDR=\frac{V}{R}$$`

---

## False Positives &amp; False Discoveries

The __false positive rate__ is:
`$$FPR=\frac{number\:of\:falsely\:rejected\:H_0s}{total\:number\:of\:true\:H_0s}$$`

The __false discovery rate__ is:
`$$FDR=\frac{number\:of\:falsely\:rejected\:H_0s}{total\:number\:of\:rejected\:H_0s}$$`

Benjamini-Hochberg proposed to keep the latter less than `\(\alpha\)` such that the maximum FDR is capped at `\(q&lt;\alpha\)`.

???
* How often, when the null was true, did I think I had an effect?
* Of all the times I thought I had an effect, how often was I wrong?
  * e.g. what proportion my "discoveries" are wrong? 
---

## The FDR Method

Sort your obtained *p*-values from lowest to highest, and add the following information:

* `\(p\)` = sorted unadjusted *p*-values (these can be based on any statistic, here Tukey HSD)
* `\(j\)` = variable indexing the order of that contrast in the sort ( `\(j=1,\dotsc,m\)` ) 
* `\(p_{BH}^*\)` = critical values for *p*, based on Benjamini-Hochberg per contrast adjustments holding `\(q&lt;.05\)` (see next slides)


```
     TMNT - Superman Hulk - Superman Superman - Spiderman TMNT - Spiderman
p              0.003           0.008                0.094            0.392
j              1.000           2.000                3.000            4.000
p*BH           0.008           0.017                0.025            0.033
     Hulk - Spiderman TMNT - Hulk
p               0.661       0.966
j               5.000       6.000
p*BH            0.042       0.050
```

???

.008 = .05/6
.017 = .05/3
---

## The Benjamini-Hochberg False Discovery Rate

The previous table can be summarized more generally as:

| |smallest| `\(\rightarrow\)` | `\(\rightarrow\)`  | `\(\rightarrow\)` |largest|
|:--:|:---:|:--:|:--:|:--------:|:----:|
| `\(p-values\)` | `\(p_1\)` | `\(p_2\)` | `\(p_3\)` | `\(\dotsc\)` | `\(p_m\)` |
| `\(j\)` |1|2|3| `\(\dotsc\)` | `\(m\)` |
| `\(p_{BH}^*\)` | `\(\frac{1}{m}\times{\alpha}\)` | `\(\frac{2}{m}\times{\alpha}\)` | `\(\frac{3}{m}\times{\alpha}\)` | `\(\dotsc\)` | `\(\frac{m}{m}\times{\alpha}=\alpha\)` |

Where the `\(p_{BH}^*\)` threshold for each individual contrast is:
`$$\frac{j}{m}\times{\alpha}$$`
???
m is the number of groups

---

## Decision Time



```
         TMNT - Superman Hulk - Superman Superman - Spiderman
p                  0.003           0.008                0.094
j                      1               2                    3
p*BH               0.008           0.017                0.025
Decision       REJECT H0       REJECT H0     DO NOT REJECT H0
         TMNT - Spiderman Hulk - Spiderman      TMNT - Hulk
p                   0.392            0.661            0.966
j                       4                5                6
p*BH                0.033            0.042             0.05
Decision DO NOT REJECT H0 DO NOT REJECT H0 DO NOT REJECT H0
```

---

## Bonferroni Method

A different approach to adjusting *p*-values per contrast is to control the __false positive rate__ by controlling the *family-wise error rate*= `\(\alpha\)` .

__The idea:__ if you want to control your Type I error rate at `\(\alpha=.05\)` across all *m* contrasts, then simply compare your obtained *p*-value with a new `\(p_{Bonferroni}^*\)`:
`$$p_{Bonferroni}^*=\frac{\alpha}{m}$$`

Where *m* is the maximum total number of contrasts you'll need to perform.

__The downside:__ comes at a cost of decreasing statisical power (increasing Type II errors) and thus being overly conservative

In our current example, we conduct *m*=6 contrasts total, so our nominal per contrast *p* is:
`$$p_{Bonferroni}^*=\frac{.05}{6}=.008$$`
???
* Really conservative
* Good for small n
* Other methods much better for large sets!!
---

## False Positives 

Recall that the __false positive rate__ is:
`$$FPR=\frac{number\:of\:falsely\:rejected\:H_0s}{total\:number\:of\:true\:H_0s}$$`


---

## Bonferroni vs. Benjamini-Hochberg

Let's compare to the BH FDR:

| |smallest| |  | `\(\rightarrow\)` |largest|
|:--:|:---:|:--:|:--:|:--------:|:----:|
| `\(p-values\)` | `\(p_1\)` | `\(p_2\)` | `\(p_3\)` | `\(\dotsc\)` | `\(p_m\)` |
| `\(j\)` |1|2|3| `\(\dotsc\)` | `\(m\)` |
| `\(p_{BH}^*\)` | `\(1\times{\frac{\alpha}{m}}\)` | `\(2\times{\frac{\alpha}{m}}\)` | `\(3\times{\frac{\alpha}{m}}\)` | `\(\dotsc\)` | `\(m\times{\frac{\alpha}{m}}=\alpha\)` |
| `\(p_{Bonferroni}^*\)` | `\(\frac{\alpha}{m}\)` | `\(\frac{\alpha}{m}\)` | `\(\frac{\alpha}{m}\)` | `\(\dotsc\)` | `\(\frac{\alpha}{m}\)` |

&gt; So for the smallest *p*-value, `\(p_{BH}^*\)` will always equal `\(p_{Bonferroni}^*\)`

---

## Bonferroni vs. Benjamini-Hochberg

Would our decisions have been any different if, instead of controlling the *false discovery rate* via Benjamini-Hochberg, we controlled the *family-wise error rate* via Bonferroni?


```
                     TMNT - Superman  Hulk - Superman Superman - Spiderman
p                              0.003            0.008                0.094
j                                  1                2                    3
p*BH                           0.008            0.017                0.025
Decision: BH               REJECT H0        REJECT H0     DO NOT REJECT H0
p*Bonferroni                   0.008            0.008                0.008
Decision: Bonferroni       REJECT H0 DO NOT REJECT H0     DO NOT REJECT H0
                     TMNT - Spiderman Hulk - Spiderman      TMNT - Hulk
p                               0.392            0.661            0.966
j                                   4                5                6
p*BH                            0.033            0.042             0.05
Decision: BH         DO NOT REJECT H0 DO NOT REJECT H0 DO NOT REJECT H0
p*Bonferroni                    0.008            0.008            0.008
Decision: Bonferroni DO NOT REJECT H0 DO NOT REJECT H0 DO NOT REJECT H0
```


---

## Adjusted p-values options

We just covered two *p* value adjustment options: 

* Benjamini-Hochberg (in R, "BH" or "fdr")
* Bonferroni
* But there are many many more...

From *multcomp*:
&gt; "Shaffer" implements Bonferroni-adjustments taking logical constraints into account Shaffer [1986] and "Westfall" takes both logical constraints and correlations among the z statistics into account Westfall [1997]. In addition,
all adjustment methods implemented in p.adjust can be specified as well.

From *help(p.adjust)*:
&gt; c("holm", "hochberg", "hommel", "bonferroni", "BH", "BY", "fdr", "none")

---

## An example:

&gt; * As discussed in Benjamini and Yekutieli (2001), Needleman et al (New England Journal of Medicine, 300, 689–695) studied the neuropsychologic effects of unidentified childhood exposure to lead by comparing various
psychological and classroom performances between two groups of children differing in the lead level observed in their shed teeth. 

&gt; * While there is no doubt that high levels of lead are harmful, Needleman’s findings regarding exposure to low lead levels, especially because of their contribution to the Environmental Protection Agency’s review of lead exposure
standards, are controversial.

&gt; * The study was attacked on the grounds of methodological flaws, because they analyzed three independent "outcome" variables (or DVs)

&gt; 1. Teacher Behavioral Ratings
&gt; 2. WISC Scores
&gt; 3. Verbal Processing and Reaction Time Scores

---

## An example

Inputting the *p*-values...


```r
Teacher &lt;- sort(c(0.003,0.05,0.05,0.14, 0.08,0.01,0.04,0.01,.050,0.003,0.003))
WISC &lt;- sort(c(0.04,0.05,0.02,0.49,0.08,0.36,0.03,0.38,0.15,0.90,0.37,0.54))
RT &lt;- sort(c(0.002,0.03,0.07,0.37,0.90,0.42,0.05,0.04, 0.32,0.001,0.001,0.01))
```

Now what happens if we treat analyses for each of the 3 DVs as one "family", setting the FWER=.05 for each family?

---

## Bonferroni


```r
bonf.teacher &lt;- p.adjust(Teacher, method="bonferroni")
bonf.wisc &lt;- p.adjust(WISC, method="bonferroni")
bonf.rt &lt;- p.adjust(RT, method="bonferroni")
```

---

## Bonferroni

Here, how many would we reject for each family?
&gt; * Teacher? WISC? RT?


```r
round(bonf.teacher,2)
```

```
 [1] 0.03 0.03 0.03 0.11 0.11 0.44 0.55 0.55 0.55 0.88 1.00
```

```r
round(bonf.wisc,2)
```

```
 [1] 0.24 0.36 0.48 0.60 0.96 1.00 1.00 1.00 1.00 1.00 1.00 1.00
```

```r
round(bonf.rt,2)
```

```
 [1] 0.01 0.01 0.02 0.12 0.36 0.48 0.60 0.84 1.00 1.00 1.00 1.00
```

---

## Benjamini Hochberg


```r
bh.teacher &lt;- p.adjust(Teacher, method="BH")
bh.wisc &lt;- p.adjust(WISC, method="BH")
bh.rt &lt;- p.adjust(RT, method="BH")
```

---

## Benjamini Hochberg

Here, how many would we reject for each family?
&gt; * Teacher? WISC? RT?


```r
round(bh.teacher,2)
```

```
 [1] 0.01 0.01 0.01 0.02 0.02 0.06 0.06 0.06 0.06 0.09 0.14
```

```r
round(bh.wisc,2)
```

```
 [1] 0.15 0.15 0.15 0.15 0.19 0.30 0.51 0.51 0.51 0.59 0.59 0.90
```

```r
round(bh.rt,2)
```

```
 [1] 0.01 0.01 0.01 0.03 0.07 0.08 0.09 0.11 0.43 0.44 0.46 0.90
```

---
##Effect Size...


```r
# Pulling just the F-statistic out of the summary on the lm() model
summary(lm(injury~hero, superhero))$fstatistic
```

```
    value     numdf     dendf 
 6.715794  3.000000 20.000000 
```

```r
# Or from the aov model - a little tougher
#summary(heroaov)[[1]]$"F value"[1]
#summary(heroaov)[[1]]$"Df"
```


```r
# All the below params (except alpha) are from the above summary(s)
library(MOTE)
eta.F(3, 20, 6.716, a = .05)$eta
```

```
[1] 0.5018432
```

```r
summary(lm(injury~hero, superhero))$r.squared
```

```
[1] 0.5018355
```
--
##It's R-squared! 



&lt;!-- --- --&gt;

&lt;!-- ## (Briefly) Non-parametric omnibus --&gt;

&lt;!-- If we wanted to analyze this data using the extension of the Wilcoxon Mann Whitney Rank Sum Test, we would do a Kruskal-Wallis Rank Sum Test. --&gt;

&lt;!-- ```{r comment=NA} --&gt;
&lt;!-- kruskal.test(injury~hero,data=superhero) --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;

&lt;!-- ## (Briefly) Non-parametric contrasts --&gt;

&lt;!-- ```{r comment=NA, warning=FALSE,message=FALSE} --&gt;
&lt;!-- pairwise.wilcox.test(superhero$injury,superhero$hero,p.adj=c('BH')) --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;

&lt;!-- ## (Briefly) Non-parametric omnibus + contrasts (way better) --&gt;

&lt;!-- ```{r comment=NA, warning=FALSE,message=FALSE} --&gt;
&lt;!-- library(agricolae) --&gt;
&lt;!-- k1 &lt;- kruskal(superhero$injury,superhero$hero,alpha=.05,group=F,p.adj=c('BH')) --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;

&lt;!-- ## (Briefly) Non-parametric omnibus + contrasts (way better) --&gt;

&lt;!-- ```{r comment=NA, warning=FALSE,message=FALSE} --&gt;
&lt;!-- k1 --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;

&lt;!-- ## (Briefly) Effect size --&gt;

&lt;!-- ```{r comment=NA, warning=FALSE,message=FALSE} --&gt;
&lt;!-- library(orddom) --&gt;
&lt;!-- hulk &lt;- subset(superhero,select=("injury"),hero=="Hulk") --&gt;
&lt;!-- tnmt &lt;- subset(superhero,select=("injury"),hero=="TMNT") --&gt;
&lt;!-- orddom(hulk, tnmt,alpha=.05,paired=FALSE) --&gt;
&lt;!-- ``` --&gt;

&lt;!-- --- --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-lakeside-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
