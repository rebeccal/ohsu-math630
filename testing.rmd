---
title: "Math 530/630 CM 4.2 - lab"
subtitle: 'Bootstrapping and Confidence Intervals' 
#author: "Student Name"
#date: "`r Sys.Date()`"
output:
  html_document:
    toc: TRUE
    toc_float: TRUE
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Estimation of US male height

Suppose we want to estimate the average height of men in the U.S.

We can use data from the [BRFSS (2016)](https://www.cdc.gov/brfss/annual_data/annual_2016.html):

"The Behavioral Risk Factor Surveillance System (BRFSS) is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services."


```{r message=FALSE}
library(tidyverse)
library(skimr)
library(haven)
library(infer)
```

```{r include = TRUE}
# the data set is available on the course website, under References Plus
male_heights <- read_csv(here::here("data", "male_heights.csv"))
```

## EDA

Height is in self-reported height in centimeters.

* How many unique heights are there?
* How many men were 103 cm? 
* How about 231 cm?

```{r}
male_heights %>% 
  count(HTM4) %>% 
  arrange(desc(HTM4))
```


## Estimate 

What is the `mean` and `sd` of male heights?

```{r}
library(skimr)
skim(male_heights)
```

## Quantify precision

At this point we have an estimate of the average adult male height. We'd like to know how accurate this estimate is, and how precise. In the context of estimation, these words have a technical distinction:

"Given a set of data points from repeated measurements of the same quantity, the set can be said to be precise if the values are close to each other, while the set can be said to be accurate if their average is close to the true value of the quantity being measured."

Usually accuracy is what we really care about, but it's hard to measure accuracy unless you know the true value. And if you knew the true value, you wouldn't have to estimate it in the first place.

Quantifying precision is not as useful, but it is much easier. Here's one way to do it:

1. Use the data you have to make a model of the population.

1. Use the model to simulate the **random** data collection process.

1. Use the simulated data to compute an estimate.

1. Repeat steps 1-3 and collect the results.

To model the population, we'll use resampling; that is, we will treat the observed measurements as if they were taken from the entire population, and we will draw random samples from them.

We sample with replacement, which means that some measurements might be chosen more than once, and some might not be chosen at all. (If we sample without replacement, the resampled data is always identical to the original, so that's no good.) let's do 3 resamplings and comare them.

```{r include = TRUE}
# First, get the n (number of observations)
males_n <- male_heights %>% 
  tally() %>% 
  pull()
```

```{r inclue = TRUE}
set.seed(2051)
mh_resample1 <- male_heights %>% 
  sample_n(size = males_n, replace = TRUE)
```

In `mh_resample1`:

* How many men were 103 cm? 
* How about 231 cm?
* Try setting seed to `1903` and create `mh_resample2`, answer the two questions again.
* Try setting seed to `1701` and create `mh_resample3`, answer the two questions again.


```{r}
mh_resample1 %>% 
  count(HTM4 == 103)
```

```{r}
mh_resample1 %>% 
  count(HTM4 == 231)
```



```{r}
set.seed(1903)
mh_resample2 <- male_heights %>% 
  sample_n(size = males_n, replace = TRUE)
```


```{r}
mh_resample2 %>% 
  count(HTM4 == 103)
```

```{r}
mh_resample2 %>% 
  count(HTM4 == 231)
```


```{r}
set.seed(1701)
mh_resample3 <- male_heights %>% 
  sample_n(size = males_n, replace = TRUE)
```


```{r}
mh_resample3 %>% 
  count(HTM4 == 103)
```

```{r}
mh_resample3 %>% 
  count(HTM4 == 231)
```


```{r}
fix_windows_histograms() # If you're using windows, allows the histograms to show in the HTML output

skim(mh_resample1, HTM4)
skim(mh_resample2, HTM4)
skim(mh_resample3, HTM4)

# Make some space
remove(mh_resample1,mh_resample2,mh_resample3)
```

If we wanted to use `infer` instead to generate a SINGLE replicate or resample, we would use:

```{r eval = FALSE, include = TRUE}
set.seed(1701)
mh_resample3 <- male_heights %>% 
  specify(response = HTM4) %>% 
  generate(reps = 1, type = "bootstrap")
```


Now that we know how to create a single resample, let's generate 100 resamples, calculate the mean for each resample, and make a histogram to show the distribution of all the calculated means.

```{r}
set.seed(1701)
mh_means100 <- male_heights %>% 
  specify(response = HTM4) %>% 
  generate(reps = 100, type = "bootstrap") %>% 
  calculate(stat = "mean")

mh_means100 %>% 
  visualize()

# Removing as I go to make sure I don't run out of memory
rm(mh_means100)
```


And 300...

```{r}
set.seed(1701)
mh_means300 <- male_heights %>% 
  specify(response = HTM4) %>% 
  generate(reps = 300, type = "bootstrap") %>% 
  calculate(stat = "mean")

mh_means300 %>% 
  visualize()
```

Feel free to try more! **Warning** you might run run out of memory :0


## SEM
The width of the above distributions show how much the means vary from one resampling to the next.

We can quantify this variability by computing the standard error (e.g., the standard deviation of the sampling distribution), in this case the standard error of the mean (SEM).

```{r standard_error}
options(pillar.sigfig = 6) # Sets the number of significant digits that will display

mh_means300 %>% 
  summarize(mean_of_means = mean(stat),
            sd_of_means = sd(stat)) # this is the SE!
```
__Question:__ Why is the standard error so small?

__Question:__ Can you estimate the SEM using the standard deviations for the first few resamples?


# Confidence Intervals

We can also summarize the sampling distribution with a "confidence interval", which is a range that contains a specified fraction, like 95% (the confidence level), of the values in `mh_means`. Alternatively, we can ask for the confidence interval calculated using the standard error.

The central 95% confidence interval is between the 2.5th and 97.5th percentiles of the sampling distribution.

```{r}
# get_ci() default "confidence level" is 0.95

# Percentile method
(percentile_ci <- mh_means300 %>% 
  get_ci())

# SE method. This needs the point_estimate passed in - here that's the mean of means
mean_of_means <- mh_means300 %>% 
  summarize (mean = mean(stat))

(ci_se <- mh_means300 %>% 
  get_ci(type = "se", point_estimate = mean_of_means))
```

__Question:__ Why are they (ever so slightly) different?

__Question:__ How would you recreate these two calculations by "hand" with the help of R? 


## Visualizing the CI

You can also use `infer` to plot a histogram and shade the 95% confidence interval. Here's the confidence intervals calculated using both the percentile and SE methods.

```{r}
means_plot <- mh_means300 %>% 
  visualize() 

means_plot + 
  shade_confidence_interval(percentile_ci) +
  shade_p_value(obs_stat = mean_of_means, direction = NULL)
```

Here, 95% of the data stored in the `stat` variable in `mh_means` falls between the two endpoints with 2.5% to the left outside of the shading and 2.5% to the right outside of the shading. The cut-off points that provide our confidence interval are shown with the darker lines.

```{r}
means_plot + 
  shade_confidence_interval(ci_se) +
  shade_p_value(obs_stat = mean_of_means, direction = NULL)
```

Here, the end points are calculated using the SEM. 

__Question:__ What are the conditions for using the SE method?

## Amd more...

__Think about:__ How would the above calculations change if we used a subset of the data?


Let's try it using a subset with 2000 data elements, but resample 1000 times.

```{r}
set.seed(1701)
mh_means_samp <- sample_n(male_heights, 2000) %>% 
  specify(response = HTM4) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "mean")

mh_means_samp %>% 
  summarize(mean_of_means = mean(stat),
            sd_of_means = sd(stat)) # this is the SE!

# Percentile method
(percentile_ci <- mh_means_samp %>% 
  get_ci())

mean_means_samp <- mh_means_samp %>% 
  summarize(mean(stat))

means_plot <- mh_means_samp %>% 
  visualize() 

means_plot + 
  shade_confidence_interval(percentile_ci) +
  shade_p_value(obs_stat = mean_means_samp, direction = NULL)
```
__Question:__ What did we gain? What did we lose? What if we went smaller?










